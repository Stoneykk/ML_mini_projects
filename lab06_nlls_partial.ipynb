{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab:  Nonlinear Least-Squares for Modeling Materials\n",
    "\n",
    "In this lab, we will explore gradient descent on nonlinear least squares.  \n",
    "\n",
    "Suppose we wish to fit a model of the form,\n",
    "\n",
    "     yhat ~= f(x,w)\n",
    "     \n",
    "where `x` is a vector of features, `w` is a vector of parameters and `f` is a nonlinear function of `w`.  Often we find the parameters `w` that minimize a squared-error cost of the form \n",
    "\n",
    "     J(w) = \\sum_i (y_i - f(x_i,w))^2\n",
    "     \n",
    "where the summation is over training samples `(x_i,y_i)`.  This problem is known as nonlinear least-squares (NLLS).  In general, this optimization problem has no closed-form expression, and so gradient descent is widely used.  \n",
    "\n",
    "In this lab, we will apply NLLS to the physical modeling of materials.  Specifically, we will estimate parameters in a model for the expansion of copper as a function of temperature.  In doing this lab, you will learn to:\n",
    "* Set up a nonlinear least squares as an unconstrained optimization function\n",
    "* Compute initial parameter estimates for a simple rational model\n",
    "* Compute the gradients of the least squares objective\n",
    "* Implement gradient descent for minimizing the objective\n",
    "* Implement momentum gradient descent\n",
    "* Visualize the convergence of the algorithm\n",
    "\n",
    "We first import some key packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "The NIST agency has an excellent [nonlinear regression website](https://www.itl.nist.gov/div898/strd/nls/nls_main.shtml) that has several datasets appropriate for nonlinear regression problems.  In this lab, we will use the data from a NIST study involving the thermal expansion of copper. The response variable is the coefficient of thermal expansion, and the predictor variable is temperature in degrees kelvin.  \n",
    "\n",
    "> Hahn, T., NIST (1979), Copper Thermal Expansion Study.  (unpublished}\n",
    "\n",
    "You can download the data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y0</th>\n",
       "      <th>x0</th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.591</td>\n",
       "      <td>24.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.547</td>\n",
       "      <td>34.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.902</td>\n",
       "      <td>44.09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.894</td>\n",
       "      <td>45.07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.703</td>\n",
       "      <td>54.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y0     x0  dummy\n",
       "0  0.591  24.41    NaN\n",
       "1  1.547  34.82    NaN\n",
       "2  2.902  44.09    NaN\n",
       "3  2.894  45.07    NaN\n",
       "4  4.703  54.98    NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://itl.nist.gov/div898/strd/nls/data/LINKS/DATA/Hahn1.dat'\n",
    "df = pd.read_csv(url, skiprows=60, sep=' ',skipinitialspace=True, names=['y0','x0','dummy'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `x0` and `y0` into arrays.  Rescale `x0` and `y0` to values between `0` and `1` by dividing `x0` and `y0` by the maximum value.  Store the scaled values in vectors `x` and `y`.  The rescaling will help with the conditioning of the fitting.  Plot `y` vs. `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# x0 = ...\n",
    "# y0 = ...\n",
    "# x0 = x0/np.max(x0)\n",
    "# y0 = y0/np.max(y0)\n",
    "# plt.plot(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the problem a little more challenging, we will add some noise.  Add random Gaussian noise with mean 0 and std. dev = 0.05 to `y`.  Store the noisy results in `yn`. You can use the `np.random.normal()` function to add Gaussian noise. Plot `yn` vs. `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# yn = y + ...\n",
    "# plt.plot(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data `(x,yn)` into training and test.  Let `xtr,ytr` be training data and `xts,yts` be the test data.  You can use the `train_test_split` function.  Set `test_size=0.33` so that 1/3 of the samples are held out for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO\n",
    "# xtr, xts, ytr, yts = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Fit for a Rational Model\n",
    "\n",
    "The [NIST website](https://www.itl.nist.gov/div898/strd/nls/data/hahn1.shtml) suggests using a *rational* model of the form,\n",
    "\n",
    "      yhat = (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "      \n",
    "with `d=3`.  The model parameters are `w = [a[0],...,a[d],b[0],...,b[d-1]]`, so there are `2d+1` parameters total.    Complete the function below that takes vectors `w` and `x` and predicts a set of values `yhat` using the above model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,x):\n",
    "    \n",
    "    # Get the length\n",
    "    d = (len(w)-1)//2\n",
    "    \n",
    "    # TODO.  Extract a and b from w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "\n",
    "    # TODO.  Compute yhat.  You may use the np.polyval function\n",
    "    # But, remember you must flip the order of a and b because np.polyval outputs\n",
    "    #    a[0]*x**(d) + a[1]*x**(d-1) + ... + a[d-1]*x + a[d]\n",
    "    #\n",
    "    # arev = ...\n",
    "    # brev = ...\n",
    "    # yhat = ...\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit with a nonlinear model, most methods only obtain convergence to a *local* minimum, which is not necessarily good.  To converge to the *global* minimum, we need a good initialization.  For a rational model, one way to get a good initialization using the following trick.  First, realize that if\n",
    "\n",
    "    y ~= (a[0] + a[1]*x + ... + a[d]*x^d)/(1 + b[0]*x + ... + b[d-1]*x^d)\n",
    "    \n",
    "then, by rearranging, we get\n",
    "\n",
    "    y ~= a[0] + a[1]*x + ... + a[d]*x^d - b[0]*x*y + ... - b[d-1]*x^d*y.\n",
    "    \n",
    "Note that this latter model is not useful for *prediction* because it is predicting `y` using `y`! But it *is* useful for least-squares fitting the parameters `a` and `b` when `y` and `x` are known as training samples.  \n",
    "\n",
    "To do this least-squares fit, we can solve for parameter vector `w = [a0,a,b]` using linear regression with the newly defined `i`th feature vector\n",
    "\n",
    "    Z[i,:] = [ x[i], ... , x[i]**d, -y[i]*x[i], ... , -y[i]*x[i]**d ].\n",
    "    \n",
    "Here, `a=[a[1],...,a[d]]`, `b=[b[0],...,b[d-1]]`, and `a0` is the intercept term of the linear model.  We don't put the intercept in the `a` and `b` vectors because sklearn's `LinearRegression` method will handle it separately.\n",
    "\n",
    "As a hint for the next block, realize that a matrix with rows `[ x[i], x[i]**2, ... , x[i]**d]` can be efficiently created using broadcasting as `xtr[:,None]**powd[None,:]`, where `powd = [1,...,d]`.  For more on broadcasting, see the demo in Unit 0, especially the \"outer product\" section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 3\n",
    "\n",
    "# TODO.  Create the transformed feature matrix\n",
    "# powd = ...\n",
    "# Znum = ...  # so that Znum[i,j] = x[i]**j\n",
    "# Zden = ...  # so that Zden[i,j] = -y[i]*x[i]**j\n",
    "# Z = np.hstack((Znum, Zden))\n",
    "\n",
    "\n",
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = LinearRegression()\n",
    "# regr.fit(...)\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_ and store the parameter vector in winit\n",
    "# a0 = regr.intercept_\n",
    "# ab = regr.coef_\n",
    "# winit = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `predict` function and your initial parameter estimate `winit`, compute `yhat` for 1000 values of `x` uniformly spaced over the interval `[0,1]`.  Plot `yhat` versus `x`.  You should see that the prediction curve has some singularities.  On the same plot, superimpose the points `(xts,yts)`.  Use the `axis` command to focus your plot on the region near the data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = ...\n",
    "# yhat = ...\n",
    "# plot(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the prediction curve is that the denominator in our polynomial model for `yhat` goes to zero at certain values of `x` in `[0,1]`.  As a result, some of the `z` features become correlated, and the least-squares fit for `winit` includes relatively large coefficient values. To alleviate this problem, we can use Ridge regression instead of least-squares, in order to keep the `winit` parameters closer to zero.  Re-run the fit above with `Ridge` with `alpha = 1e-1`. You should see a much better (but not perfect) fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO.  Fit with parameters with linear regression\n",
    "# regr = Ridge(alpha=1e-1)\n",
    "# regr.fit(...)\n",
    "\n",
    "# TODO\n",
    "# Extract the parameters from regr.coef_ and regr.intercept_\n",
    "# winit = ...\n",
    "\n",
    "# TODO\n",
    "# Plot the results as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Loss Function\n",
    "\n",
    "We can now use gradient descent to improve our initial estimate of the weights `w`.  Complete the construction of the following function, which is used to compute the RSS-like cost\n",
    "\n",
    "    f(w) = 0.5*\\sum_i (y[i] - yhat[i])^2\n",
    "    \n",
    "and `fgrad`, the gradient of `f(w)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feval(w,x,y):\n",
    "    \n",
    "    # TODO.  Parse w\n",
    "    # a = ...\n",
    "    # b = ...\n",
    "\n",
    "    # TODO.  \n",
    "    # Znum = ... # so that Znum[i,j] = x[i]**j\n",
    "\n",
    "    # TODO.  \n",
    "    # Zden = ... # so that Zden[i,j] = x[i]**(j+1)\n",
    "\n",
    "    # TODO.  Compute yhat \n",
    "    # Compute the numerator and denominator using Znum, Zden, a, and b\n",
    "\n",
    "    # TODO.  Compute loss\n",
    "    # e = yhat-y\n",
    "    # f = ...\n",
    "\n",
    "    # TODO.  Compute gradients\n",
    "    # fgrad = ...\n",
    "    \n",
    "    return f, fgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your gradient computation:\n",
    "* Set `w0=winit` and compute `f0,fgrad0 = feval(w0,xtr,ytr)`\n",
    "* Make a `w1` very close to `w0` and compute `f1,fgrad1 = feval(w1,xtr,ytr)`\n",
    "* Verify that `f1-f0` is close to the predicted value based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the gradient test fails, don't move forward until you fix the problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient descent\n",
    "\n",
    "We will now try to minimize the loss function using gradient descent.  Using the function `feval` defined above, implement gradient descent.  Run gradient descent with a step size of `alpha=1e-6` starting at `w=winit`.  Run it for `nit=10000` iterations.  Compute `fgd[it]`= the objective function on iteration `it`.  Plot `fgd[it]` vs. `it` on a log-log scale.  \n",
    "\n",
    "You should see that the training loss decreases but does not fully converge after 10000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fgd = ...\n",
    "nit = 10000\n",
    "step = 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to get a faster convergence using adaptive step-size via the Armijo rule. Implement Armijo gradient descent.  Let `fadapt[it]` be the loss value that it attains on iteration `it`.  Plot `fadapt[it]` and `fgd[it]` vs. `it` on the same log-log graph.  You should see some improvement, but also some lingering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fadapt = ...\n",
    "nit = 10000\n",
    "step = 1e-6  # Initial step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the final estimate for `w` from the adaptive step-size approach, plot the predicted value of `yhat` vs. `x` for 1000 values of `x` in `[0,1]`.  On the same plot, plot `yhat` vs. `x` for the initial parameter `w=winit`.  Also, plot the test data, `yts` vs. `xts`.  You should see that gradient descent was able to improve the prediction model, although the initial prediction model was not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# xp = np.linspace(...)\n",
    "# yhat = ...\n",
    "# plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Gradient Descent\n",
    "\n",
    "This section is optional.\n",
    "\n",
    "One way to improve gradient descent is to use *momentum*.  With momentum, the gradient-descent update rule becomes:\n",
    "\n",
    "    f, fgrad = feval(w,...)\n",
    "    z = beta*z + fgrad\n",
    "    w = w - step*z\n",
    "    \n",
    "This is similar to gradient descent, except that the update direction `z` is the sum of the gradient `fgrad` and the previous update direction `z`, which tends to keep the algorithm moving in the same direction (instead of randomly changing directions, as it would if `z=fgrad`).  Implement momentum gradient-descent with `beta = 0.99` and `step=1e-5`.  Compare the convergence of this approach to plain gradient descent and the adaptive stepsize version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "nit = 1000\n",
    "step = 1e-5\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the final estimate for `w` from the momentum approach, plot the predicted value of `yhat` vs. `x` for 1000 values of `x` in `[0,1]`.  On the same plot, plot `yhat` vs. `x` for the adaptive stepsize method.  Also, plot the test data, `yts` vs. `xts`.  You should see that the momentum approach gives a slightly different prediction model than the adaptive-stepsize approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# plot yhat vs. x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
